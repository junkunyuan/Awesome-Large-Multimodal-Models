# Awesome Large Multimodal Models
[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

This repository is a collection of useful things about **Large Multimodal Models (LMMs)**. 

Stars, suggestions, and contributions are all welcome.

## Contents
- [Awesome Large Multimodal Models](#awesome-large-multimodal-models)
  - [Contents](#contents)
  - [Papers](#papers)
    - [Survey](#survey)
    - [Understanding and Analysis](#understanding-and-analysis)
    - [Multi-Modal Foundation Models](#multi-modal-foundation-models)
    - [Multi-Modal Instruction Tuning](#multi-modal-instruction-tuning)
    - [Multi-Modal Reinforcement Learning with Human Feedback (RLHF)](#multi-modal-reinforcement-learning-with-human-feedback-rlhf)
  - [Acknowledgement](#acknowledgement)

## Papers

### Survey

| submit_date | title | publication | code | else |
| --- | --- | --- | --- | --- |
| 2023-06-23 | [A Survey on Multimodal Large Language Models](https://arxiv.org/abs/2306.13549) | arXiv_2023 | ![Star](https://img.shields.io/github/stars/BradyFU/Awesome-Multimodal-Large-Language-Models.svg?style=social&label=Star) | - |

### Understanding and Analysis

| submit_date | title | publication | code | else |
| --- | --- | --- | --- | --- |
| 2023-09-29 | [The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)](https://arxiv.org/abs/2309.17421) | arXiv_2023 | - | - |

### Multi-Modal Foundation Models

| submit_date | title | publication | code | else |
| --- | --- | --- | --- | --- |
| 2023-09-25 | [GPT-4V(ision) system card](https://openai.com/research/gpt-4v-system-card?ref=www.chatgpt-vision.com) (*GPT-4V*) | OpenAI_2023 | - | - |
| 2023-03-15 | [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774) (*GPT-4*) | arXiv_2023 | - | - |

### Multi-Modal Instruction Tuning

| submit_date | title | publication | code | else |
| --- | --- | --- | --- | --- |
| 2023-10-5 | [Improved Baselines with Visual Instruction Tuning](https://arxiv.org/abs/2310.03744) (*LLaVA-1.5*) | arXiv_2023 | ![Star](https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&label=Star) | [demo](https://llava.hliu.cc/) |
| 2023-04-27 | [mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality](https://arxiv.org/abs/2304.14178) (*mPLUG-Owl*) | arXiv_2023 | ![Star](https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl.svg?style=social&label=Star) | [demo](https://huggingface.co/spaces/MAGAer13/mPLUG-Owl) |
| 2023-04-20 | [MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models](https://arxiv.org/abs/2304.10592) (*MiniGPT-4*) | arXiv_2023 | ![Star](https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4.svg?style=social&label=Star) | [demo](https://huggingface.co/spaces/Vision-CAIR/minigpt4) 
| 2023-04-17 | [Visual Instruction Tuning](https://browse.arxiv.org/abs/2304.08485) (*LLaVA*) | NeurIPS_2023 | ![Star](https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&label=Star) | [demo](https://llava.hliu.cc/) |

### Multi-Modal Reinforcement Learning with Human Feedback (RLHF)
| submit_date | title | publication | code | else |
| --- | --- | --- | --- | --- |
| 2023-09-25 | [Aligning Large Multimodal Models with Factually Augmented RLHF](https://arxiv.org/abs/2309.14525) (*LLaVA-RLHF*) | arXiv_2023 | ![Star](https://img.shields.io/github/stars/llava-rlhf/LLaVA-RLHF.svg?style=social&label=Star) | [demo](http://pitt.lti.cs.cmu.edu:7890/) |

<!-- ### Multi-Modal In-Context Learning -->

<!-- ### Multi-Modal Chain-of-Thought -->

<!-- ### LLM-Driven Visual Reasoning -->

<!-- ### Evaluation -->

<!-- ### Others -->

<!-- ## Datasets -->

## Acknowledgement

We referred to [Awesome-Multimodal-Large-Language-Models](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models).